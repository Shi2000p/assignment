{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc32fc4-14e4-4f00-b7eb-91e34d059e77",
   "metadata": {},
   "source": [
    "# Logistic Regression-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce6709d-6f77-4220-84ea-1a4527cffac8",
   "metadata": {},
   "source": [
    "## Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01855e66-9684-4df7-9bbe-0d3623ab507f",
   "metadata": {},
   "source": [
    "In Machine Learning project, we train different models on the dataset and select the one with the best performance. However, there is room for improvement as we cannot say for sure that this particular model is best for the problem at hand. Hence, our aim is to improve the model in any way possible. One important factor in the performances of these models are their hyperparameters, once we set appropriate values for these hyperparameters, the performance of a model can improve significantly. In this article, we will find out how we can find optimal values for the hyperparameters of a model by using GridSearchCV.\n",
    "\n",
    "GridSearchCV is the process of performing hyperparameter tuning in order to determine the optimal values for a given model. As mentioned above, the performance of a model significantly depends on the value of hyperparameters. Note that there is no way to know in advance the best values for hyperparameters so ideally, we need to try all possible values to know the optimal values. Doing this manually could take a considerable amount of time and resources and thus we use GridSearchCV to automate the tuning of hyperparameters.\n",
    "\n",
    "GridSearchCV tries all the combinations of the values passed in the dictionary and evaluates the model for each combination using the Cross-Validation method. Hence after using this function we get accuracy/loss for every combination of hyperparameters and we can choose the one with the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6921c3-34a6-47df-9b81-0ae1ad05abc9",
   "metadata": {},
   "source": [
    "## Q2. Describe the difference between grid search cv and randomize search cv, and when might you chooseone over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941be11f-5a10-4c0b-b3ef-56659a708c49",
   "metadata": {},
   "source": [
    "Both are technique to find the right set of Hyper-Parameter to achieve high Precision and Accuracy for any model or algorithm in Machine Learning , Deep Learning any where .\n",
    "Hyper-Parameter are model specific setting that are fixed to build a model so such model is trained or tested on the data and give higher Accuracy.\n",
    "\n",
    "GridSearchCV:-GridSearchCV is checking by combination of all parameter in a sequence.\n",
    "\n",
    "RandomizedSearchCV:-RandomizedSearchCV is checking Random combination based on given Attrivute n_iter value.\n",
    "\n",
    "GridSearchCV:-when there are less rows of data then we go for GridSearchCV \n",
    "\n",
    "RandomizedSearchCV:-when there is very Big data set then we go for RandomizedSearchCV.\n",
    "\n",
    "GridSearchCV is slower than RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6106b0e9-c970-4f5b-a7fd-3ada58af3a57",
   "metadata": {},
   "source": [
    "## Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0171cc32-024d-4886-839a-de7a634e6583",
   "metadata": {},
   "source": [
    "data leakage happens when data ends up somewhere it’s not supposed to be. In machine learning, data leakage may cause overly optimistic or invalid predictive models. Data leaks can also cause significant data security issues when data that’s supposed to be protected, is instead exposed.\n",
    "\n",
    "Data leaks of both types are the result of errors made during model creation or during configuration. An example is when data from the test dataset is shared with the training dataset, causing a machine learning model to perform better than it should, because it already knows the data that’s being presented. Then, when working with real data, the performance is worse or may produce inaccurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269260c4-8158-4153-b827-c013e35b5140",
   "metadata": {},
   "source": [
    "## Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2e1dc3-79aa-47a4-83ca-13069a001ab8",
   "metadata": {},
   "source": [
    "1.Split Your Data Sets:= Split your datasets into training, validation, and test datasets, and hold back the validation dataset for a final check of your developed models.\n",
    "\n",
    "2.Simplify access where possible:= If it’s too hard for employees to access the systems they need to do their work, they find ways around the security, including such things as downloading sensitive data to work offline. To make things manageable for your employees, reduce complexity.\n",
    "\n",
    "3.User multi-factor authentication:= Using more than just a username and password protects against credential loss, and it also simplifies password management and other access controls. This multi-factor authentication includes steps such as sending a security code to a device such as a cell phone. \n",
    "\n",
    "4.Train your staff:= Your employees need to understand what data leakage is, why preventing it is important, and what they can do to stem it. Training should cover topics including why it’s not a good idea to copy data onto unauthorized devices, why they shouldn’t write their credentials on notes, and why they need to check the addresses of the emails they send out.\n",
    "\n",
    "5.Consider email content filtering:= Security appliances designed to examine and detect unauthorized material in outgoing email work in conjunction with your firewall to prevent sensitive data from being exfiltrated. This can be part of a data loss prevention solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5a5f60-bf2d-4a0f-97d7-26f0b3e4939d",
   "metadata": {},
   "source": [
    "## Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acbc2db-693b-4493-a513-a851504fd06d",
   "metadata": {},
   "source": [
    "A confusion matrix is a matrix that summarizes the performance of a machine learning model on a set of test data. It is often used to measure the performance of classification models, which aim to predict a categorical label for each input instance. The matrix displays the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) produced by the model on the test data.\n",
    "For binary classification, the matrix will be of a 2X2 table, For multi-class classification, the matrix shape will be equal to the number of classes i.e for n classes it will be nXn. \n",
    "\n",
    "From the confusion matrix, we can find the following metrics:=\n",
    "\n",
    "Accuracy:  Accuracy is used to measure the performance of the model. It is the ratio of Total correct instances to the total instances. \n",
    "Accuracy= {TP+TN}/{TP+TN+FP+FN}\n",
    "\n",
    "Precision: Precision is a measure of how accurate a model’s positive predictions are. It is defined as the ratio of true positive predictions to the total number of positive predictions made by the model\n",
    "Precision ={TP}/{TP+FP}\n",
    "\n",
    "Recall: Recall measures the effectiveness of a classification model in identifying all relevant instances from a dataset. It is the ratio of the number of true positive (TP) instances to the sum of true positive and false negative (FN) instances.\n",
    "Recall ={TP}/{TP+FN}\n",
    "\n",
    "F1-Score: F1-score is used to evaluate the overall performance of a classification model. It is the harmonic mean of precision and recall,\n",
    "F1-Score ={2*Precision*Recall}/{Precision + Recall}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1652e90f-2d86-447f-87b3-55e7ba829db1",
   "metadata": {},
   "source": [
    "## Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d1244-55c4-45c4-a987-6a4dc84488e0",
   "metadata": {},
   "source": [
    "‍Precision is the ratio of true positives to the total of the true positives and false positives. Precision looks to see how much junk positives got thrown in the mix. If there are no bad positives (those FPs), then the model had 100% precision. The more FPs that get into the mix, the uglier that precision is going to look.\n",
    "Precision ={TP}/{TP+FP}\n",
    "\n",
    "Recall goes another route. Instead of looking at the number of false positives the model predicted, recall looks at the number of false negatives that were thrown into the prediction mix.\n",
    "Recall = TP/(TP + FN)\n",
    "\n",
    "Precision is defined as the proportion of the positive class predictions that were actually correct. In other words, if a model classified a total of 100 samples to be of positive class, and 70 of them actually belonged to the positive class of the dataset (and 30 were negative class samples predicted incorrectly as “positive” by the classifier), then the precision is 70%.\n",
    "\n",
    "Recall is defined as the proportion of actual positive class samples that were identified by the model. That is if the test set of a dataset consists of 100 samples in its positive class, how many of them were identified? If 60 of the positive samples were identified correctly, then the recall is 60%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4f36e4-c19c-43d0-be00-7473a20f10be",
   "metadata": {},
   "source": [
    "## Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551907e3-4a19-4f41-a9ae-da3131e6a74d",
   "metadata": {},
   "source": [
    "The confusion matrix shows the ways in which your classification model is confused when it makes predictions. It gives you insight not only into the errors being made by your classifier but more importantly the types of errors that are being made. It is this breakdown that overcomes the limitation of using classification accuracy alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff9900-f9e6-4d0d-bd9d-afaf3f60ebe4",
   "metadata": {},
   "source": [
    "## Q8. What are some common metrics that can be derived from a confusion matrix, and how are theycalculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d95e741-de77-4210-b2d8-707d2b5f2e53",
   "metadata": {},
   "source": [
    "Confusion Matrix, the following derived metrics are :-\n",
    "\n",
    "Precision: Precision is a measure of how accurate a model’s positive predictions are. It is defined as the ratio of true positive predictions to the total number of positive predictions made by the model Precision ={TP}/{TP+FP}\n",
    "\n",
    "Recall: Recall measures the effectiveness of a classification model in identifying all relevant instances from a dataset. It is the ratio of the number of true positive (TP) instances to the sum of true positive and false negative (FN) instances. Recall ={TP}/{TP+FN}\n",
    "\n",
    "F1-Score: F1-score is used to evaluate the overall performance of a classification model. It is the harmonic mean of precision and recall, F1-Score ={2PrecisionRecall}/{Precision + Recall}\n",
    "\n",
    "Micro Average is an average of true positive, false positive, and false negative while working with a multiclass classification problem.\n",
    "Precision, Recall, and F1-Score all have Micro Average.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a484fdd1-1673-433a-bf1d-9f87afc6d8d3",
   "metadata": {},
   "source": [
    "## Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a007e1-c2f6-4766-95ba-59cd0a84ea63",
   "metadata": {},
   "source": [
    "A confusion matrix is a matrix that summarizes the performance of a machine learning model on a set of test data. It is often used to measure the performance of classification models, which aim to predict a categorical label for each input instance. The matrix displays the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) produced by the model on the test data.\n",
    "\n",
    "Accuracy:  Accuracy is used to measure the performance of the model. It is the ratio of Total correct instances to the total instances. \n",
    "\n",
    "Accuracy ={TP+TN}/{TP+TN+FP+FN}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a89f22-1e86-49fb-8942-e45fe903ff9e",
   "metadata": {},
   "source": [
    "## Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learningmodel?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06367dae-d134-469b-979e-24d67d1ce3a9",
   "metadata": {},
   "source": [
    "The confusion matrix shows the ways in which your classification model is confused when it makes predictions. It gives you insight not only into the errors being made by your classifier but more importantly the types of errors that are being made. It is this breakdown that overcomes the limitation of using classification accuracy alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1194b60b-e5cc-4658-9d96-f1f8b4280fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
