{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9448bdb1-453c-499b-86fe-623ee0c85337",
   "metadata": {},
   "source": [
    "# Regression-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be835b8-f5ba-43c6-9162-74ec90d032e7",
   "metadata": {},
   "source": [
    "## Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ef314f-6847-43e0-a0f8-6473fc18fd24",
   "metadata": {},
   "source": [
    "Lasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of muticollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination.\n",
    "The acronym “LASSO” stands for Least Absolute Shrinkage and Selection Operator.\n",
    "\n",
    "Lasso regression performs L1 regularization, which adds a penalty equal to the absolute value of the magnitude of coefficients. This type of regularization can result in sparse models with few coefficients; Some coefficients can become zero and eliminated from the model. Larger penalties result in coefficient values closer to zero, which is the ideal for producing simpler models. On the other hand, L2 regularization (e.g. Ridge regression) doesn’t result in elimination of coefficients or sparse models. This makes the Lasso far easier to interpret than the Ridge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1979ca7d-c253-4482-8880-fa0362356147",
   "metadata": {},
   "source": [
    "## Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a21dd7-a7a5-4671-b939-1b98b8a5d519",
   "metadata": {},
   "source": [
    " A particular advantage with this technique is that it reduces overfitting without restricting a subset of the dataset to sole use for internal validation.LASSO regression has been shown to outperform standard methods in some settings.\n",
    " However, it is not a panacea to the problems of overfitting and optimism bias, and does not remove the need to validate a model in an external dataset. Additionally, the LASSO approach trades off potential bias in estimating individual parameters for a better expected overall prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77160a99-4f80-4c9b-a32a-11cdcea30cfd",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6cc747-208c-4111-8ac9-e55da2a6e469",
   "metadata": {},
   "source": [
    "As λ increases, more and more coefficients are set to zero and eliminated (theoretically, when λ = ∞, all coefficients are eliminated). As λ increases, bias increases. As λ decreases, variance increases. If an intercept is included in the model, it is usually left unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41464cf2-aa13-4633-8e80-ee5dfd313b2b",
   "metadata": {},
   "source": [
    "## Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b4bda-e505-4797-93e0-3346fb12a9c7",
   "metadata": {},
   "source": [
    "The essential part of LASSO is just adding an L 1 norm of the coefficients to the main term, f (x, y, β) + λ ‖ β ‖ 1. There's no reason f has to be a linear model. It may not have an analytic solution, or be convex, but there's nothing stopping you from trying it out, and it should still induce sparsity, contingent on a large enough lambda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f836c5-b704-480a-a452-17c7a2c8d614",
   "metadata": {},
   "source": [
    "## Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41f1bac-298c-4869-b672-a62a1e8a13b3",
   "metadata": {},
   "source": [
    "Ridge Regression :-\n",
    "\n",
    "In Ridge regression, we add a penalty term which is equal to the square of the coefficient. The L2 term is equal to the square of the magnitude of the coefficients. We also add a coefficient  \\lambda  to control that penalty term. In this case if  \\lambda  is zero then the equation is the basic OLS else if  \\lambda \\, > \\, 0 then it will add a constraint to the coefficient. As we increase the value of \\lambda this constraint causes the value of the coefficient to tend towards zero. This leads to tradeoff of higher bias (dependencies on certain coefficients tend to be 0 and on certain coefficients tend to be very large, making the model less flexible) for lower variance.\n",
    "\n",
    "Lasso Regression :-\n",
    "\n",
    "Lasso regression stands for Least Absolute Shrinkage and Selection Operator. It adds penalty term to the cost function. This term is the absolute sum of the coefficients. As the value of coefficients increases from 0 this term penalizes, cause model, to decrease the value of coefficients in order to reduce loss. The difference between ridge and lasso regression is that it tends to make coefficients to absolute zero as compared to Ridge which never sets the value of coefficient to absolute zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ecdf34-13df-47bf-8b1c-d584982bc4f3",
   "metadata": {},
   "source": [
    "## Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfef146-a806-4e3d-8dcf-24c4ab189840",
   "metadata": {},
   "source": [
    "Multicollinearity is a phenomenon in which two or more predictors in a multiple regression are highly correlated (R-squared more than 0.7), this can inflate our regression coefficients. We can test multicollinearity with the Variance Inflation Factor VIF is the ratio of variance in a model with multiple terms, divided by the variance of a model with one term alone. VIF = 1/1-R-squared. A rule of thumb is that if VIF > 10 then multicollinearity is high (a cutoff of 5 is also commonly used).\n",
    "To reduce multicollinearity we can use regularization that means to keep all the features but reducing the magnitude of the coefficients of the model. This is a good solution when each predictor contributes to predict the dependent variable.\n",
    "\n",
    "LASSO Regression is similar to RIDGE REGRESSION except to a very important difference. The Penalty Function now is: lambda*|slope|\n",
    "The result is very similar to the result given by the Ridge Regression. Both can be used in logistic regression, regression with discrete values and regression with interaction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8932ee4-2cb7-4155-a852-e5b0c71536f7",
   "metadata": {},
   "source": [
    "## Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d3845f-eddb-4587-974b-01bb33ddf15b",
   "metadata": {},
   "source": [
    "Once we determine that lasso regression is appropriate to use, we can fit the model (using popular programming languages like R or Python) using the optimal value for λ. To determine the optimal value for λ, we can fit several models using different values for λ and choose λ to be the value that produces the lowest test MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e0211e-d205-4014-83ba-c4079bac8553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
