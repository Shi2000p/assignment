{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb40e256-88de-472f-8019-fb67300eb681",
   "metadata": {},
   "source": [
    "### Machine Learning-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297524ca-f9bb-475c-8cb4-c0a46c67f0d8",
   "metadata": {},
   "source": [
    "#### Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84584394-c467-4448-8ec9-50448de1e1a9",
   "metadata": {},
   "source": [
    "#### Underfitting\n",
    "Underfitting:- A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture the underlying trend of the data,it only performs well on training data but performs poorly on testing data.Underfitting destroys the accuracy of our machine learning model. Its occurrence simply means that our model or the algorithm does not fit the data well enough. Underfitting refers to a model that can neither performs well on the training data nor generalize to new data.\n",
    "\n",
    "Reasons for Underfitting:-\n",
    "\n",
    "1.High bias and low variance.\n",
    "\n",
    "2.The size of the training dataset used is not enough.\n",
    "\n",
    "3.The model is too simple.\n",
    "\n",
    "4.Training data is not cleaned and also contains noise in it.\n",
    "\n",
    "Techniques to reduce underfitting:-\n",
    "\n",
    "1.Remove noise from the data.\n",
    "\n",
    "2.Increase the number of epochs or increase the duration of training to get better results.\n",
    "\n",
    "3.Increase model complexity.\n",
    "\n",
    "4.Increase the number of features, performing feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e9fe10-ca2d-4142-ab06-79bc844c22a6",
   "metadata": {},
   "source": [
    "### overfitting\n",
    "\n",
    "Overfitting: A statistical model is said to be overfitted when the model does not make accurate predictions on testing data. When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. And when testing with test data results in High variance. Then the model does not categorize the data correctly, because of too many details and noise.\n",
    "Overfitting is a problem where the evaluation of machine learning algorithms on training data is different from unseen data.\n",
    "\n",
    "Reasons for Overfitting are as follows:-\n",
    "\n",
    "1.High variance and low bias.\n",
    "\n",
    "2.The model is too complex.\n",
    "\n",
    "3.The size of the training data.\n",
    "\n",
    "Techniques to reduce overfitting:-\n",
    "\n",
    "1.Increase training data.\n",
    "\n",
    "2.Reduce model complexity.\n",
    "\n",
    "3.Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to\n",
    "increase stop training).\n",
    "\n",
    "4.Ridge Regularization and Lasso Regularization\n",
    "\n",
    "5.Use dropout for neural networks to tackle overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e50b7-af76-42e4-91a4-f95a7d9578be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d08f8e5a-635a-4af0-a523-3ffaab71bd68",
   "metadata": {},
   "source": [
    "#### Q2: How can we reduce overfitting? Explain in brief"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8f06ba-56ec-4331-8a25-ff1b143668ea",
   "metadata": {},
   "source": [
    "1.Cross validation:-\n",
    "\n",
    "The most robust method to reduce overfitting is collect more data. The more data we have, the easier it is to explore and model the underlying structure.\n",
    "\n",
    "In a machine learning workflow, we split the data into training and test subsets. In some cases, we also put aside a separate set for validation. The model is trained on the training set. Then, its performance is measured on the test set. Thus, we evaluate the model on previously unseen data.\n",
    "\n",
    "In this scenario, we cannot use a portion of the dataset for training. We are kind of wasting it. Cross validation allows for using every observation in both training and test sets.\n",
    "\n",
    "2.Regularization;-\n",
    "\n",
    "If a model is too complex with respect to the data, it is highly likely to result in overfitting.Regularization is a method for reducing the complexity. It controls the model complexity by adding a penalty for higher terms. Normally, a model aims to minimize the loss according to the given loss function. If a regularization terms is added, the model tries to minimize both the complexity and loss.\n",
    "\n",
    "3.Ensemble models:-\n",
    "\n",
    "Ensemble models consist of many small (i.e. weak) learners. The overall model tends to be more robust and accurate than the individual ones. The risk of overfitting also decreases when we use ensemble models.\n",
    "\n",
    "The most commonly used ensemble models are random forest and gradient boosted decision trees. They are a combination of several decision trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9929928b-d7a4-4bd4-9d2d-39dc51232c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cda917c2-6744-49cd-9de3-5b3a965fc189",
   "metadata": {},
   "source": [
    "### Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "Underfitting:- A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture the underlying trend of the data,it only performs well on training data but performs poorly on testing data.Underfitting destroys the accuracy of our machine learning model. Its occurrence simply means that our model or the algorithm does not fit the data well enough. Underfitting refers to a model that can neither performs well on the training data nor generalize to new data.\n",
    "\n",
    "Reasons for Underfitting:-\n",
    "\n",
    "1.High bias and low variance.\n",
    "\n",
    "2.The size of the training dataset used is not enough.\n",
    "\n",
    "3.The model is too simple.\n",
    "\n",
    "4.Training data is not cleaned and also contains noise in it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf9d65-5d8f-4cd3-b2c5-e73311885622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44535e51-2e19-43e7-9211-5a73775258a4",
   "metadata": {},
   "source": [
    "#### Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356aab6f-26c3-4870-89b4-731f1a4d85b4",
   "metadata": {},
   "source": [
    "Bias:-\n",
    "\n",
    "The bias is known as the difference between the prediction of the values by the ML model and the correct value. Being high in biasing gives a large error in training as well as testing data. Its recommended that an algorithm should always be low biased to avoid the problem of underfitting.\n",
    "\n",
    "Variance:-\n",
    "\n",
    "The variability of model prediction for a given data point which tells us spread of our data is called the variance of the model. The model with high variance has a very complex fit to the training data and thus is not able to fit accurately on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data.\n",
    "When a model is high on variance, it is then said to as Overfitting of Data\n",
    "\n",
    "There can be four combinations between bias and variance:-\n",
    "\n",
    "1.High Bias, Low Variance: A model with high bias and low variance is said to be underfitting.\n",
    "\n",
    "2.High Variance, Low Bias: A model with high variance and low bias is said to be overfitting.\n",
    "\n",
    "3.High-Bias, High-Variance: A model has both high bias and high variance, which means that the model is not able to capture the underlying patterns in the data (high bias) and is also too sensitive to changes in the training data (high variance). As a result, the model will produce inconsistent and inaccurate predictions on average.\n",
    "\n",
    "4.Low Bias, Low Variance: A model that has low bias and low variance means that the model is able to capture the underlying patterns in the data (low bias) and is not too sensitive to changes in the training data (low variance). This is the ideal scenario for a machine learning model, as it is able to generalize well to new, unseen data and produce consistent and accurate predictions. But in practice, it’s not possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a978e05-a0b2-4b1b-91ec-fce075809060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e732378-4b80-4a2c-9267-490e63e334a8",
   "metadata": {},
   "source": [
    "#### Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c33ee-9fa2-4a52-9738-adc31b187806",
   "metadata": {},
   "source": [
    "There are several ways to detect over- or under-fitting in a machine learning model:-\n",
    "\n",
    "1.Plot the learning curves: Learning curves show the model’s performance on training and validation data over time as the model is being trained. If the model is overfitting, you will see that the training error continues to decrease over time, while the validation error starts to increase after a certain point. This indicates that the model is beginning to memorise the training data and needs to be generalised well to new, unseen data.\n",
    "\n",
    "2.Evaluate the model on a holdout set: A holdout set is a subset of the data that is not used during training but is used to evaluate the model after training. If the model performs well on the training data but poorly on the holdout set, it may be overfitting the training data.\n",
    "\n",
    "3.Use cross-validation: Cross-validation is a technique where the data is divided into k-folds, and the model is trained and evaluated on each fold. If the model performs well on the training data but poorly on the validation data, it may need to be more balanced.\n",
    "\n",
    "4.Regularise the model: Regularization is a method that adds a penalty term to the loss function to stop the model from becoming too similar to the training data. By changing the regularisation parameter, you can control how hard the model is to understand and prevent it from becoming too simple.\n",
    "\n",
    "5.Use simpler models: If your complex model is overfitting the data, you can use simpler models less prone to overfitting, such as linear models or decision trees with low depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97301cc4-03ed-43bc-8f00-04c6e6646b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16ad4fab-0d0a-440a-b04e-23045a982fd2",
   "metadata": {},
   "source": [
    "#### Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940217c4-3e19-4525-9948-550a040dacaf",
   "metadata": {},
   "source": [
    "COMPARISION:-\n",
    "\n",
    "Definition;\n",
    "\n",
    "BIAS:-When an algorithm is employed in a machine learning model and it does not fit well, a phenomenon known as bias can develop. Bias arises in several situations.\n",
    "\n",
    "VARIANCE:-The term \"variance\" refers to the degree of change that may be expected in the estimation of the target function as a result of using multiple sets of training data.\n",
    "\n",
    "Values;\n",
    "\n",
    "BIAS:-The disparity between the values that were predicted and the values that were actually observed is referred to as bias.\n",
    "\n",
    "VARIANCE:-A random variable's variance is a measure of how much it varies from the value that was predicted for it.\n",
    "\n",
    "Data;\n",
    "\n",
    "BIAS:-The model is incapable of locating patterns in the dataset that it was trained on, and it produces inaccurate results for both seen and unseen data.\t\n",
    "\n",
    "VARIANCE:-The model recognizes the majority of the dataset's patterns and can even learn from the noise or data that isn't vital to its operation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ca4b0-d3a1-495e-89f7-e3e15674c319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f662c457-f4c7-47e0-91c5-168b2617d24f",
   "metadata": {},
   "source": [
    "#### Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6144f5-eab9-4d81-96b0-5512311c7b03",
   "metadata": {},
   "source": [
    "Regularization is an application of Occam’s Razor. It is one of the key concepts in Machine learning as it helps choose a simple model rather than a complex one.Regularization refers to the modifications that can be made to a learning algorithm that helps to reduce this generalization error and not the training error. It reduces by ignoring the less important features. It also helps prevent overfitting, making the model more robust and decreasing the complexity of a model.\n",
    "\n",
    "regularization techniques:-\n",
    "\n",
    "a. Ridge Regression:-\n",
    "\n",
    "The Ridge regression technique is used to analyze the model where the variables may be having multicollinearity. It reduces the insignificant independent variables though it does not remove them completely. This type of regularization uses the L2 norm for regularization.\n",
    "\n",
    "b. Lasso Regression:-\n",
    "\n",
    "Least Absolute Shrinkage and Selection Operator (or LASSO) Regression penalizes the coefficients to the extent that it becomes zero. It eliminates the insignificant independent variables. This regularization technique uses the L1 norm for regularization.\n",
    "\n",
    "c. Elastic Net Regression:-\n",
    "\n",
    "The Elastic Net Regression technique is a combination of the Ridge and Lasso regression technique. It is the linear combination of penalties for both the L1-norm and L2-norm regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be09798-bc25-409e-81f6-8fd6fd22f9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
