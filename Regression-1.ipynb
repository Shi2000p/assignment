{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "148893bd-3f13-42cd-8522-9c164d5fb7e7",
   "metadata": {},
   "source": [
    "# Regression-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53df2b-88d1-4cb0-b254-ccaa94c7d0c7",
   "metadata": {},
   "source": [
    "## Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06a286a-685e-4b5b-85a2-21eed0c1a6a9",
   "metadata": {},
   "source": [
    "Simple Linear Regression:-\n",
    "\n",
    "Simple Linear Regression establishes the relationship between two variables using a straight line. It attempts to draw a line that comes closest to the data by finding the slope and intercept which define the line and minimize regression errors. Simple linear regression has only one x and one y variable.\n",
    "\n",
    "example:-You are a social researcher interested in the relationship between income and happiness. You survey 500 people whose incomes range from 15k to 75k and ask them to rank their happiness on a scale from 1 to 10.\n",
    "independent variable (income) and dependent variable (happiness) are both quantitative, we can do a regression analysis to see if there is a linear relationship between them.\n",
    "\n",
    "Multi Linear Regression:-\n",
    "Multiple Linear regressions are based on the assumption that there is a linear relationship between both the dependent and independent variables or Predictor variable and Target variable. It also assumes that there is no major correlation between the independent variables. Multi Linear regressions can be linear and nonlinear. It has one y and two or more x variables or one dependent variable and two or more independent variables. \n",
    "\n",
    "example:- real estate employee who wants to create a model to help predict the best time to sell homes. You hope to sell homes at the maximum sales price, but multiple factors can affect the sales price. These variables include the age of the house, the value of other homes in the neighborhood, quantitative measurements of the public school system regarding student performance and the number of nearby parks, among other factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d39f8c1-0dbf-414b-a65c-3a2f14e1542a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b30eb2b9-73cf-4163-94dc-e000b8e05eba",
   "metadata": {},
   "source": [
    "## Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ec9d2-7de1-41dc-b070-95897a4f3b42",
   "metadata": {},
   "source": [
    "Mainly there are 7 assumptions taken while using Linear Regression:-\n",
    "\n",
    "1.Linear Model:-According to this assumption, the relationship between the independent and dependent variables should be linear. \n",
    "\n",
    "2.No Multicolinearlity in the data:-If the predictor variables are correlated among themselves, then the data is said to have a multicollinearity problem.high collinearity means that the two variables vary very similarly and contain the same kind of information. This will leads to redundancy in the dataset. Due to redundancy, only the complexity of the model increase, and no new information or pattern is learned by the model. We generally try to avoid highly correlated features even while using complex models.\n",
    "\n",
    "3.Homoscedasticity of Residuals or Equal Variances:-\n",
    "Homoscedasity is the term that states that the spread residuals which we are getting from the linear regression model should be homogeneous or equal spaces. If the spread of the residuals is heterogeneous then the model is called to be an unsatisfactory model.\n",
    "\n",
    "4.No Autocorrelation in residuals:-\n",
    "One of the critical assumptions of multiple linear regression is that there should be no autocorrelation in the data. When the residuals are dependent on each other, there is autocorrelation. This factor is visible in the case of stock prices when the price of a stock is not independent of its previous one.\n",
    "\n",
    "5.Number of observations Greater than the number of predictors:-\n",
    "For a better-performing model, the number of training data or observations should be always greater than the number of test or prediction data. However greater the number of observations better the model performance. Therefore, to build a linear regression model you must have more observations than the number of independent variables (predictors) in the data set. \n",
    "\n",
    "6.Each observation is unique:-\n",
    "It is also important to ensure that each observation is independent of the other observation.  Meaning each observation in the data set should be measured separately on a unique occurrence of the event that caused the observation.\n",
    "\n",
    "7.Predictors are distributed Normally:-\n",
    "This assumption ensures that you have equally distributed observations for the range of each predictor. So at the end of the model training, the predicted values for each test data should be a normal distribution. One can get an idea of the distribution of the predicted values by plotting density, KDE, or QQ plots for the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429b496-21f0-4290-8806-21f015b54435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b90f097-36ed-461a-a98e-4b392db0ce56",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5ec1c2-ca85-4c61-95f1-47c9e31ad32e",
   "metadata": {},
   "source": [
    "The slope indicates the steepness of a line and the intercept indicates the location where it intersects an axis. The slope and the intercept define the linear relationship between two variables, and can be used to estimate an average rate of change. The greater the magnitude of the slope, the steeper the line and the greater the rate of change.\n",
    "\n",
    "The intercept (sometimes called the “constant”) in a regression model represents the mean value of the response variable when all of the predictor variables in the model are equal to zero.\n",
    "\n",
    "Example 1: Intercept Makes Sense to Interpret:-\n",
    "\n",
    "Suppose we’d like to fit a simple linear regression model using hours studied as a predictor variable and exam score as the response variable.\n",
    "We collect this data for 50 students in a certain college course and fit the following regression model:\n",
    "Exam score = 65.4 + 2.67(hours)\n",
    "The value for the intercept term in this model is 65.4. This means the average exam score is 65.4 when the number of hours studied is equal to zero.This makes sense to interpret since it’s plausible for a student to study for zero hours in preparation for an exam.\n",
    "\n",
    "Example 2: Intercept Does Not Make Sense to Interpret\n",
    "\n",
    "Suppose we’d like to fit a simple linear regression model using weight (in pounds) as a predictor variable and height (in inches) as the response variable.We collect this data for 50 individuals and fit the following regression model:\n",
    "Height = 22.3 + 0.28(pounds)\n",
    "The value for the intercept term in this model is 22.3. This would mean the average height of a person is 22.3 inches when their weight is equal to zero.This does not make sense to interpret since it’s not possible for a person to weigh zero pounds.However, we still need to keep the intercept term in the model in order to use the model to make predictions. The intercept just doesn’t have any meaningful interpretation for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271b3fc8-5773-400b-aba3-2ce4cb0d005e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4e2162b-1e33-468f-aa5f-2b53b6134c93",
   "metadata": {},
   "source": [
    "## Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f4e3d8-b07e-4fc3-80d6-05a3d1d0f05d",
   "metadata": {},
   "source": [
    "Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks.  Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates. Until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error. Once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (AI) and computer science applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3202634-76ee-4066-b118-21ca21ca8e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faa55dba-0aca-4dfb-bf43-741ebe11de03",
   "metadata": {},
   "source": [
    "## Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56195804-b806-40c9-8104-c5c81495456f",
   "metadata": {},
   "source": [
    "Multiple linear regression refers to a statistical technique that is used to predict the outcome of a variable based on the value of two or more variables. It is sometimes known simply as multiple regression, and it is an extension of linear regression. The variable that we want to predict is known as the dependent variable, while the variables we use to predict the value of the dependent variable are known as independent or explanatory variables.\n",
    "\n",
    "Multiple linear regression is a more specific calculation than simple linear regression. For straight-forward relationships, simple linear regression may easily capture the relationship between the two variables. For more complex relationships requiring more consideration, multiple linear regression is often better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eed0d1-640a-4e4b-a3c0-febe380d8a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1878840c-4d4e-4802-96be-a16b109da1ed",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04b1c3c-afe5-4415-bfe6-7baffacf589d",
   "metadata": {},
   "source": [
    "Multicollinearity is the occurrence of high intercorrelations among two or more independent variables in a multiple regression model. Multicollinearity can lead to skewed or misleading results when a researcher or analyst attempts to determine how well each independent variable can be used most effectively to predict or understand the dependent variable in a statistical model.\n",
    "In general, multicollinearity can lead to wider confidence intervals that produce less reliable probabilities in terms of the effect of independent variables in a model.\n",
    "\n",
    "A statistical technique called the variance inflation factor (VIF) can detect and measure the amount of collinearity in a multiple regression model. VIF measures how much the variance of the estimated regression coefficients is inflated as compared to when the predictor variables are not linearly related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dc4316-a446-427e-ba7a-ccc79b986136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbc3b11e-dfa4-4025-8cf2-fc14b02cdbbd",
   "metadata": {},
   "source": [
    "## Q7. Describe the polynomial regression model. How is it different from linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e7de41-5427-4cfe-aab0-5f35605797f4",
   "metadata": {},
   "source": [
    "Polynomial regression is a type of regression analysis that is used when the relationship between the dependent and independent variables is not linear. It involves fitting a polynomial function to the data points to obtain a curve that represents the relationship between the variables.\n",
    "\n",
    "difference between linear and polynomial regression:-\n",
    "\n",
    "1.Linearity:-\n",
    "\n",
    "LINEAR REGRESSION:-Assumes linear relationship between dependent and independent variables.\n",
    "\n",
    "POLYNOMIAL REGRESSION:-Does not assume linear relationship.\n",
    "\n",
    "2.Equation:-\n",
    "\n",
    "LINEAR REGRESSION:-Uses a straight line equation to represent the relationship between variables\t\n",
    "\n",
    "POLYNOMIAL REGRESSION:-Uses a polynomial equation to represent the relationship.\n",
    "\n",
    "3.Complexity:-\n",
    "\n",
    "LINEAR REGRESSION:-Simple, only involves fitting a straight line\t\n",
    "\n",
    "POLYNOMIAL REGRESSION:-More complex, involves fitting a polynomial function\n",
    "\n",
    "4.Overfitting\t:-\n",
    "\n",
    "LINEAR REGRESSION:-Less prone to overfitting.\n",
    "\n",
    "POLYNOMIAL REGRESSION:-Prone to overfitting.\n",
    "\n",
    "5.Assumptions:-\n",
    "\n",
    "LINEAR REGRESSION:-Assumes residuals are normally distributed\n",
    "\n",
    "POLYNOMIAL REGRESSION:-Assumes residuals are randomly distributed\n",
    "\n",
    "Data Points:-\n",
    "\n",
    "LINEAR REGRESSION:-Suitable for few data points\t\n",
    "\n",
    "POLYNOMIAL REGRESSION:-Suitable for many data points\n",
    "\n",
    "Extrapolation:-\n",
    "\n",
    "LINEAR REGRESSION:-Suitable for extrapolation\t\n",
    "\n",
    "POLYNOMIAL REGRESSION:-Not suitable for extrapolation\n",
    "\n",
    "Computation:-\n",
    "\n",
    "LINEAR REGRESSION:-Quick computation\n",
    "\n",
    "POLYNOMIAL REGRESSION:-Requires more computation power\n",
    "\n",
    "Accuracy:-\n",
    "\n",
    "LINEAR REGRESSION:-Less accurate in modeling non-linear relationships\n",
    "\n",
    "POLYNOMIAL REGRESSION:-More accurate in modeling non-linear relationships\n",
    "\n",
    "Non-linear relationships:-\n",
    "\n",
    "LINEAR REGRESSION:-Not suitable for modeling non-linear relationships\t\n",
    "\n",
    "POLYNOMIAL REGRESSION:-Suitable for modeling non-linear relationships\n",
    "\n",
    "Goodness of Fit:-\n",
    "\n",
    "LINEAR REGRESSION:-Provides a good fit for linear relationships\t\n",
    "\n",
    "POLYNOMIAL REGRESSION:-Provides a better fit for non-linear relationships\n",
    "\n",
    "Sensitivity to outliers\t:-\n",
    "\n",
    "LINEAR REGRESSION:-Less sensitive to outliers\n",
    "\n",
    "POLYNOMIAL REGRESSION:-More sensitive to outliers\n",
    "\n",
    "Application:-\t\n",
    "\n",
    "LINEAR REGRESSION:-Used for continuous dependent variables with linear relationships\t\n",
    "\n",
    "POLYNOMIAL REGRESSION:-Used for non-linear relationships between variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065071a1-0313-444a-b4a5-6eefc9421f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62593415-68d4-4e5c-81db-aae1ef0d90c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
