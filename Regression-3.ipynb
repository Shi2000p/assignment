{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a1e8b5f-47c3-4f30-a268-9cf7660f9223",
   "metadata": {},
   "source": [
    "# Regression-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c8e64-ba75-49d1-9ce7-ed4c584d3893",
   "metadata": {},
   "source": [
    "## Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a501b1fc-5f9b-4e5c-9cc4-c3ad9e8b1d7d",
   "metadata": {},
   "source": [
    "Ridge Regression is an adaptation of the popular and widely used linear regression algorithm. It enhances regular linear regression by slightly changing its cost function, which results in less overfit models.\n",
    "it is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values.\n",
    "\n",
    "The cost function for ridge regression:=Min(||Y – X(theta)||^2 + λ||theta||^2)\n",
    "\n",
    "Lambda is the penalty term. λ given here is denoted by an alpha parameter in the ridge function. So, by changing the values of alpha, we are controlling the penalty term. The higher the values of alpha, the bigger is the penalty and therefore the magnitude of coefficients is reduced.\n",
    "\n",
    "It shrinks the parameters. Therefore, it is used to prevent multicollinearity\n",
    "It reduces the model complexity by coefficient shrinkage\n",
    "Check out the free course on regression analysis.\n",
    "\n",
    "The biggest benefit of ridge regression is its ability to produce a lower test mean squared error (MSE) compared to least squares regression when multicollinearity is present. However, the biggest drawback of ridge regression is its inability to perform variable selection since it includes all predictor variables in the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea62f7f7-697f-4f5c-b04b-9cc1c9d662c5",
   "metadata": {},
   "source": [
    "## Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a27fe28-28c0-4487-81cd-2173d5d87e10",
   "metadata": {},
   "source": [
    "A tuning parameter (λ), sometimes called a penalty parameter, controls the strength of the penalty term in ridge regression and lasso regression. It is basically the amount of shrinkage, where data values are shrunk towards a central point, like the mean. Shrinkage results in simple, sparse models which are easier to analyze than high-dimensional data models with large numbers of parameters.\n",
    "\n",
    "When λ = 0, no parameters are eliminated. The estimate is equal to the one found with linear regression.\n",
    "As λ increases, more and more coefficients are set to zero and eliminated.\n",
    "When λ = ∞, all coefficients are eliminated.\n",
    "\n",
    "There is a trade-off between bias and variance in resulting estimators. As λ increases, bias increases and as λ decreases, variance increases. For example, setting your tuning parameter to a low value results in a more manageable number of model parameters and lower bias, but at the expense of a much larger variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd211c0-1792-4a1e-961e-39c80a992ed2",
   "metadata": {},
   "source": [
    "## Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe8c54-73fc-43fd-803f-36f6cc2941c3",
   "metadata": {},
   "source": [
    "We can use ridge regression for feature selection while fitting the model. In this article, we are going to use logistic regression for model fitting and push the parameter penalty as L2 which basically means the penalty we use in ridge regression. ridge_logit =LogisticRegression (C=1, penalty='l2') ridge_logit.fit (X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc490e0-62bb-4249-9b64-17f5caa03a77",
   "metadata": {},
   "source": [
    "## Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91abc6f5-cfba-4cdf-a7b3-3bdfef01b7b4",
   "metadata": {},
   "source": [
    "Ridge regression decreases the parameters of low contributing variables towards zero, but not exactly to zero, and stabilizing the parameter variance of the least squares estimator in the presence of multicollinearity.\n",
    "Author: Hyoshin Kim, Hye-Young Jung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf9cc7-9992-455a-ba03-d314302571d9",
   "metadata": {},
   "source": [
    "## Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46569bb3-02ba-45f7-8315-3e8406e4bcad",
   "metadata": {},
   "source": [
    "For a continuous predictor variable, the regression coefficient represents the difference in the predicted value of the response variable for each one-unit change in the predictor variable, assuming all other predictor variables are held constant.\n",
    "For a categorical predictor variable, the regression coefficient represents the difference in the predicted value of the response variable between the category for which the predictor variable = 0 and the category for which \n",
    "the predictor variable = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ab43b4-92a2-4359-aaf6-f85203a3efe5",
   "metadata": {},
   "source": [
    "## Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0306081-929a-48fb-a7f3-55a5ad9ee117",
   "metadata": {},
   "source": [
    "Time series data is a type of data where you record each observation at a specific point in time. You also collect the observations at regular intervals. In time series data, the order of the observations matters, and you use the data to analyze changes or patterns.\n",
    "\n",
    "Examples of this type of data include stock prices, weather measurements, economic indicators, and many others. Time series \n",
    "data is commonly used in various fields, including finance, economics, engineering, and social sciences.\n",
    "\n",
    "Yes, we can run a regression on time series data. In time series regression, the dependent variable is a time series, and the independent variables can be other time series or non-time series variables.\n",
    "\n",
    "Time series regression helps you understand the relationship between variables over time and forecast future values of the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3108a2f2-7acf-4815-86eb-8cf6d2435483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
