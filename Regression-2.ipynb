{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad29d-4b26-4335-a0cd-c51d76bf80a4",
   "metadata": {},
   "source": [
    "# Regression-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1f4a89-c550-4797-bdf1-94bb3435c5d9",
   "metadata": {},
   "source": [
    "## Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e0fc51-a38b-4ca5-a7b4-87adc4b3469c",
   "metadata": {},
   "source": [
    "R-squared is a statistical measure that represents the goodness of fit of a regression model. The value of R-square lies between 0 to 1. Where we get R-square equals 1 when the model perfectly fits the data and there is no difference between the predicted value and actual value.  However, we get R-square equals 0 when the model does not predict any variability in the model and it does not learn any relationship between the dependent and independent variables.\n",
    "\n",
    "How is it calculated:-\n",
    "\n",
    "1.First, calculate the mean of the target/dependent variable y and we denote it by y̅.\n",
    "\n",
    "2.Calculate the total sum of squares by subtracting each observation yi from y̅, then squaring it and summing these square differences across all the values. It is denoted by  SStot = \\sum_{i=1}^{n} (y_i - \\bar{y})^2.\n",
    "\n",
    "2.We estimate the model parameter using a suitable regression model such as Linear Regression or SVM Regressor.\n",
    "\n",
    "3.We calculate the Sum of squares due to regression which is denoted by SSR. This is calculated by subtracting each predicted value of y denoted by y_predi from yi squaring these differences and then summing all the n terms. \n",
    "SSR = \\sum_{i=1}^{n} (\\hat{ypred}_i – \\bar{y})^2.\n",
    "\n",
    "4.We calculate the sum of squares (SSres). It explains unaccounted variability in the dependent y after predicting these values from an independent variable in the model.  SSres = \\sum_{i=1}^n (y_i – {ypred}_i)^2\n",
    "\n",
    "5.we can then use either R^2 = \\frac{SSR}{SStot}  or R^2 = 1 - \\frac{SSres}{SStot}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b690f1-235a-4d2b-b406-7f1bef864bee",
   "metadata": {},
   "source": [
    "## Q2. Define adjusted R-squared and explain how it differs from the regular R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70acd01-50bf-442e-8256-0cf46fe39288",
   "metadata": {},
   "source": [
    "Adjusted R squared is a statistical tool that measures the accuracy of a regression model12. It adjusts for the number of independent variables that explain the variation of the dependent variable. It increases when a new variable improves the model more than expected by chance, and decreases when a variable does not improve the model enough. It helps investors compare the goodness-of-fit for different models.\n",
    "\n",
    "Adjusted R-Squared is an updated version of R-squared which takes account of the number of independent variables while calculating R-squared. The main problem with R-squared is that the R-Square value always increases with an increase in independent variables irrespective of the fact that where the independent variable is contributing to the model or not. This leads to the model having high variance if the model has a lot of independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142f6cf-9bc9-4faf-9b88-9b01a6521ecd",
   "metadata": {},
   "source": [
    "## Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a00af3-bf32-49bc-bd5a-d01757882a7a",
   "metadata": {},
   "source": [
    "Adjusted R-squared tells us how well a set of predictor variables is able to explain the variation in the response variable, adjusted for the number of predictors in a model. Because of the way it’s calculated, adjusted R-squared can be used to compare the fit of regression models with different numbers of predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f35af-1ae9-4a19-bf0c-546861825d1e",
   "metadata": {},
   "source": [
    "## Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb91fc0-a89a-43b9-9896-3e682766f18c",
   "metadata": {},
   "source": [
    "Regression analysis is a technique we can use to understand the relationship between one or more predictor variables and a response variable. \n",
    "\n",
    "### RMSE:-\n",
    "One way to assess how well a regression model fits a dataset is to calculate the root mean square error, which is a metric that tells us the average distance between the predicted values from the model and the actual values in the dataset.\n",
    "The lower the RMSE, the better a given model is able to “fit” a dataset.\n",
    "The formula to find the root mean square error, often abbreviated RMSE, is as follows:=RMSE = √Σ(Pi – Oi)2 / n\n",
    "\n",
    ":-Σ is a fancy symbol that means “sum”\n",
    ":-Pi is the predicted value for the ist observation in the dataset\n",
    ":-Oi is the observed value for the ist observation in the dataset\n",
    ":-n is the sample size\n",
    "\n",
    "### MSE:-\n",
    "A metric that tells us the average squared difference between the predicted values and the actual values in a dataset. The lower the MSE, the better a model fits a dataset.\n",
    "\n",
    "MSE = Σ(ŷi – yi)2 / n\n",
    "\n",
    ":-Σ is a symbol that means “sum”\n",
    ":-ŷi is the predicted value for the ith observation\n",
    ":-yi is the observed value for the ith observation\n",
    ":-n is the sample size\n",
    "\n",
    "### MAE:-\n",
    "Mean Absolute Error is the average absolute error between actual and predicted values.\n",
    "Absolute error, also known as L1 loss, is a row-level error calculation where the non-negative difference between the prediction and the actual is calculated. MAE is the aggregated mean of these errors, which helps us understand the model performance over the whole dataset.\n",
    "MAE is a popular metric to use as the error value is easily interpreted. This is because the value is on the same scale as the target you are predicting for..\n",
    "The closer MAE is to 0, the more accurate the model is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dba09c3-c46e-4d2e-8984-90fb67c25488",
   "metadata": {},
   "source": [
    "## Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522d7e8b-0676-4a26-985f-f5e3a6fd619b",
   "metadata": {},
   "source": [
    "### MSE:-\n",
    "ADVANTAGES:-\n",
    "\n",
    "    1.Equation is differentiable.\n",
    "    2.It has only one local or global minima.\n",
    "    \n",
    "DISADVANTAGE:-\n",
    "\n",
    "    1.Not robust to outliers.\n",
    "    2.It is not in the same unit.\n",
    "    \n",
    "### MAE:-\n",
    "ADVANTAGES:-\n",
    "\n",
    "    1.It robust to outlier.\n",
    "    2.It will be in the same unit.\n",
    "    \n",
    "DISADVANTAGE:-\n",
    "\n",
    "    1.Convergence usually takes more time.\n",
    "    \n",
    "### RMSE:-\n",
    "ADVANTAGES:-\n",
    "\n",
    "    1.difference is in same unit.\n",
    "    2.It is differenciable ,only one global minima.\n",
    "    \n",
    "DISADVANTAGE:-\n",
    "\n",
    "    1.Not robust to outliers.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7678841a-7dbc-438c-8f79-083bec4dc60a",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0466d89b-900e-4125-86f7-fbcd302daad0",
   "metadata": {},
   "source": [
    "Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of multicollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination.Lasso Regression uses L1 regularization technique (will be discussed later in this article). It is used when we have more features because it automatically performs feature selection.\n",
    "\n",
    "Lasso performs better if there are small number of independent features or significant parameters i.e. when only a few predictors actually influence the response Ridge works well if there are large number of independent features or significant parameters i.e. when most predictors impact the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be61e2-e425-4498-bb86-6c91548ea902",
   "metadata": {},
   "source": [
    "## Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bd5968-e813-4e73-9bc1-97601e4567b6",
   "metadata": {},
   "source": [
    "Regularization means restricting a model to avoid overfitting by shrinking the coefficient estimates to zero. When a model suffers from overfitting, we should control the model's complexity. Technically, regularization avoids overfitting by adding a penalty to the model's loss function:= REGULARISATION = LOSS FUNCTION + PENALTY\n",
    "\n",
    "we use regularization techniques to fix overfitting in our machine learning models.\n",
    "Overfitting happens when a machine learning model fits tightly to the training data and tries to learn all the details in the data; in this case, the model cannot generalize well to the unseen data.\n",
    "a high variance machine learning model captures all the details of the training data along with the existing noise in the data. So, as you've seen in the generalization curve, the difference between training loss and validation loss is becoming more and more noticeable. On the contrary, a high bias machine learning model is loosely coupled to the training data, which leads to a low difference between training loss and validation loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa014060-b023-4d90-9719-7bdf80e27167",
   "metadata": {},
   "source": [
    "## Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e9584e-76ea-4cfb-9de1-ef977afa453d",
   "metadata": {},
   "source": [
    "I choose model B MAE of 8 .because A metric that tells us the mean absolute difference between the predicted values and the actual values in a dataset. The lower the MAE, the better a model fits a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d460f5d-613b-49c3-ade6-490e4c88f0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f412ac75-db64-435c-bd20-4a9f09bd952b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6e92e2-d1a8-4f3c-a0bb-329c2d295a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
